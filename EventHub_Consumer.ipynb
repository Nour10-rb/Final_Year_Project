{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bb3e048-b8c7-40aa-b60f-01730ebd2e1d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "import mlflow\n",
    "import mlflow.azureml\n",
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.authentication import ServicePrincipalAuthentication\n",
    "from azureml.core.webservice import AciWebservice, Webservice\n",
    "from pyspark.sql.types import *\n",
    "import  pyspark.sql.functions as F\n",
    "import requests\n",
    "import json\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aed49b30-fb4d-492c-8dad-9d8989231bd2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "cs='Endpoint=sb://eventstream.servicebus.windows.net/;SharedAccessKeyName=datastreaming;SharedAccessKey=OrMKgUFa4/Nsg7KP+MpFJx5oMecC8OsyV+AEhEk9v7I='\n",
    "connection_string=cs+\";EntityPath=eventhub1\"\n",
    "conf={}\n",
    "conf['eventhubs.connectionstring']=sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(connection_string)\n",
    "conf['eventhubs.consumerGroup'] = \"$Default\"\n",
    "input_stream_df=(spark.readStream.format(\"eventhubs\").options(**conf).option(\"startingOffsets\", \"earliest\").load())\n",
    "input_stream_df= input_stream_df.withColumn(\"body\", input_stream_df[\"body\"].cast(\"string\"))\n",
    "input_stream_df.awaitTermination(10)\n",
    "input_stream_df.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f0ea3ed-cded-4d24-9375-ec68e85b7314",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json\n",
    "\n",
    "# Créer une session Spark\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "# Définir le schéma du JSON\n",
    "json_schema = \"ID_INFO_ACTION INT, SYMBOL STRING, LANGUAGE STRING, DISPLAY_NAME STRING, EXCHANGETIMEZONENAME STRING\"\n",
    "\n",
    "cs = 'Endpoint=sb://eventstream.servicebus.windows.net/;SharedAccessKeyName=datastreaming;SharedAccessKey=OrMKgUFa4/Nsg7KP+MpFJx5oMecC8OsyV+AEhEk9v7I='\n",
    "connection_string = cs + \";EntityPath=eventhub1\"\n",
    "\n",
    "conf = {}\n",
    "conf['eventhubs.connectionString'] = sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(connection_string)\n",
    "conf['eventhubs.consumerGroup'] = \"$Default\"\n",
    "\n",
    "input_stream_df = spark.readStream.format(\"eventhubs\").options(**conf).option(\"startingOffsets\", \"earliest\").load()\n",
    "input_stream_df = input_stream_df.withColumn(\"body\", input_stream_df[\"body\"].cast(\"string\"))\n",
    "\n",
    "# Écrire le flux de sortie dans la console\n",
    "streaming_query = input_stream_df.writeStream.format(\"console\").option(\"truncate\", False).start()\n",
    "\n",
    "# Attendre la terminaison du streaming\n",
    "streaming_query.awaitTermination(10)\n",
    "\n",
    "# Arrêter manuellement le streaming\n",
    "streaming_query.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aff3d15f-aa9f-4fd2-819f-b25de33f77e6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "import json\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Define the schema of the JSON\n",
    "json_schema = StructType([\n",
    "    StructField(\"Date\", StringType()),\n",
    "    StructField(\"Open\", DoubleType()),\n",
    "    StructField(\"High\", DoubleType()),\n",
    "    StructField(\"Low\", DoubleType()),\n",
    "    StructField(\"Close\", DoubleType()),\n",
    "    StructField(\"Adj Close\", DoubleType()),\n",
    "    StructField(\"Volume\", IntegerType()),\n",
    "    StructField(\"company_name\", StringType())\n",
    "])\n",
    "\n",
    "cs = 'Endpoint=sb://eventstream.servicebus.windows.net/;SharedAccessKeyName=eventstream;SharedAccessKey=LUTLKiPfZ6DL5N7WGDuNL67fXmj4I/AqU+AEhKyDYJw='\n",
    "connection_string = cs + \";EntityPath=eventhub1\"\n",
    "\n",
    "conf = {}\n",
    "conf['eventhubs.connectionString'] = sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(connection_string)\n",
    "conf['eventhubs.consumerGroup'] = \"$Default\"\n",
    "\n",
    "# Load streaming data from eventhubs\n",
    "input_stream_df = spark.readStream.format(\"eventhubs\").options(**conf).option(\"startingOffsets\", \"earliest\").load()\n",
    "\n",
    "\n",
    "# Convert the \"body\" column to JSON structure\n",
    "body_df = input_stream_df.select(from_json(col(\"body\").cast(\"string\"), json_schema).alias(\"body_json\"))\n",
    "\n",
    "# Extract the JSON fields into separate columns\n",
    "body_df = body_df.select(\n",
    "    col(\"body_json.Date\"),\n",
    "    col(\"body_json.Open\"),\n",
    "    col(\"body_json.High\"),\n",
    "    col(\"body_json.Low\"),\n",
    "    col(\"body_json.Close\"),\n",
    "    col(\"body_json.`Adj Close`\").alias(\"Adj_Close\"),\n",
    "    col(\"body_json.Volume\"),\n",
    "    col(\"body_json.company_name\")\n",
    ")\n",
    "input_stream = generaterow(body_df)\n",
    "# Convert the \"Date\" column to a date type\n",
    "#body_df = body_df.withColumn(\"Date\", col(\"Date\").cast(\"date\"))\n",
    "\n",
    "# Write the streaming DataFrame to the console\n",
    "query = body_df.writeStream.format(\"console\").outputMode(\"append\").start()\n",
    "\n",
    "# Wait for user input to stop the streaming query\n",
    "input(\"Press Enter to stop the streaming query...\")\n",
    "\n",
    "# Stop the streaming query\n",
    "query.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99c79d7a-dddf-4a81-9ae0-7f46b7c13982",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Define the schema of the JSON\n",
    "json_schema = StructType([\n",
    "    StructField(\"Date\", StringType()),\n",
    "    StructField(\"Open\", DoubleType()),\n",
    "    StructField(\"High\", DoubleType()),\n",
    "    StructField(\"Low\", DoubleType()),\n",
    "    StructField(\"Close\", DoubleType()),\n",
    "    StructField(\"Adj Close\", DoubleType()),\n",
    "    StructField(\"Volume\", IntegerType()),\n",
    "    StructField(\"company_name\", StringType())\n",
    "])\n",
    "\n",
    "cs = 'Endpoint=sb://eventstream.servicebus.windows.net/;SharedAccessKeyName=eventstream;SharedAccessKey=nHmzl5uUAsHTGktmNJ3EFqzSlEwC4meLH+AEhHqg460='\n",
    "connection_string = cs + \";EntityPath=eventhub1\"\n",
    "\n",
    "conf = {}\n",
    "conf['eventhubs.connectionString'] = sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(connection_string)\n",
    "conf['eventhubs.consumerGroup'] = \"$Default\"\n",
    "\n",
    "# Load streaming data from eventhubs\n",
    "input_stream_df = spark.readStream.format(\"eventhubs\").options(**conf).option(\"startingOffsets\", \"earliest\").load()\n",
    "\n",
    "# Convert the \"body\" column to JSON structure\n",
    "body_df = input_stream_df.select(from_json(col(\"body\").cast(\"string\"), json_schema).alias(\"body_json\"))\n",
    "\n",
    "# Extract the JSON fields into separate columns\n",
    "body_df = body_df.select(\n",
    "    col(\"body_json.Date\"),\n",
    "    col(\"body_json.Open\"),\n",
    "    col(\"body_json.High\"),\n",
    "    col(\"body_json.Low\"),\n",
    "    col(\"body_json.Close\"),\n",
    "    col(\"body_json.`Adj Close`\").alias(\"Adj_Close\"),\n",
    "    col(\"body_json.Volume\"),\n",
    "    col(\"body_json.company_name\")\n",
    ")\n",
    "event_body = input_stream_df.body_as_str().decode('utf-8')\n",
    "event_data = json.loads(event_body)\n",
    "new_row_df = generaterow(event_data)\n",
    "new_row_df.show(truncate=False)\n",
    "    \n",
    "# Save the DataFrame to the database table\n",
    "new_row_df.write.format(\"jdbc\").option('url', url).option(\"dbtable\", \"fait_test\").mode(\"append\").save()\n",
    "# Convert the \"Date\" column to a date type\n",
    "#body_df = body_df.withColumn(\"Date\", col(\"Date\").cast(\"date\"))\n",
    "\n",
    "# Write the streaming DataFrame to the console\n",
    "#query = body_df.writeStream.format(\"delta\").outputMode(\"append\").option(\"checkpointLocation\", \"/FileStore/tables\").start(\"/FileStore/tables\")\n",
    "\n",
    "# Wait for user input to stop the streaming query\n",
    "input(\"Press Enter to stop the streaming query...\")\n",
    "\n",
    "# Stop the streaming query\n",
    "#query.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77dd770f-8c14-4721-861f-13a90e9a15cf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "body_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5126a3cc-64d9-4be3-b72c-6a6983cc5f3b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "write stream data in azure sql database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b1b9c7f-2072-41ac-ae53-5d9ccad50b84",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "test update via library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8526965c-eaa7-40ee-b27e-1a53767f656a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "578fe77c-1cf2-45a8-80b0-cadd100daf54",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "1-library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88571490-1a01-407c-9316-ea86cebc56d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import yfinance as yf\n",
    "yf.pdr_override()\n",
    "from pandas_datareader import data as pdr\n",
    "#import fix_yahoo_finance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26c588c4-72c4-4bbf-ae4e-e3e367b4e883",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaa82c89-92a1-4123-a37f-f3f43dbf2a5e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "driver = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "\n",
    "database_host = \"portefeuille.database.windows.net\"\n",
    "database_port = \"1433\" # update if you use a non-default port\n",
    "database_name = \"portefeuille\"\n",
    "table = \"DIM_DATE\"\n",
    "user = \"portefeuille\"\n",
    "password = \"Noursharm_123\"\n",
    "url=\"jdbc:sqlserver://portefeuille.database.windows.net:1433;database=portefeuille;user=portefeuille@portefeuille;password=Noursharm_123;encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.database.windows.net;loginTimeout=30;\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0c53a1d-d266-4089-8e09-e760d542c447",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def checkandaddskdate(new_date_id, dim_date, data):\n",
    "    # Check if ID_DATE already exists in dim_date DataFrame\n",
    "    existing_date = dim_date.filter(col(\"ID_DATE\") == new_date_id).count()\n",
    "    \n",
    "    # If ID_DATE does not exist, add the new row to the dim_date DataFrame and save it to \"DIM_DATE_test\"\n",
    "    if existing_date == 0:\n",
    "        new_date_row = {\n",
    "            \"ID_DATE\": new_date_id,\n",
    "            \"YEAR\": int(new_date_id[:4]),\n",
    "            \"MONTH\": int(new_date_id[4:6]),\n",
    "            \"DAY\": int(new_date_id[6:])\n",
    "        }\n",
    "        \n",
    "        # Create the new row DataFrame\n",
    "        new_dm_date_df = spark.createDataFrame([new_date_row])\n",
    "        \n",
    "        # Write the new row DataFrame to \"DIM_DATE_test\" table\n",
    "        new_dm_date_df.write.format(\"jdbc\").option(\"driver\", driver).option(\"url\", url).option(\"dbtable\", \"DIM_DATE\").mode(\"append\").save()\n",
    "        \n",
    "    return dim_date\n",
    "\n",
    "#new_date_id = \"20230407\"\n",
    "#updated_dim_date_df = checkandaddskdate(new_date_id, dim_date, data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a8ad517-593f-4238-9bc8-5f544541085b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def checkandaddidinfoaction(new_info_id,dim_info,data):\n",
    "    existing_action = dim_info.filter(col(\"SYMBOL\") == data[\"company_name\"]).count()\n",
    "    # Check and add new rows to the dimension table  brand\n",
    "    \n",
    "    if existing_action == 0:\n",
    "        new_action_row = {\n",
    "        \"ID_INFO_ACTION\": new_info_id,\n",
    "        \"SYMBOL\":data[\"company_name\"],\n",
    "        \"LANGUAGE\":\"\",\n",
    "        \"DISPLAY_NAME\":\"\",\n",
    "        \"EXCHANGETIMEZONENAME\":\"\"}\n",
    "        # create the new line for the update\n",
    "        new_dim_info =spark.createDataFrame([new_action_row])\n",
    "        # Write the updated dimension data back to the respective table\n",
    "        new_dim_info.write.format(\"jdbc\").option(\"driver\", driver).option(\"url\", url).option(\"dbtable\", \"DIM_INFO_ACT\").mode(\"append\").save()\n",
    "#new_info_id =dim_info.selectExpr(\"MAX(ID_INFO_ACTION) + 1\").first()[0]\n",
    "#updated_dim_action_df = checkandaddidinfoaction(new_info_id,dim_info,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d9f248e-f39e-41db-ab2d-7ebc1067a5d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#new_revenue_id =dim_revenue.selectExpr(\"MAX(ID_REVENUE) + 1\").first()[0]\n",
    "def checkandaddidrevenue(new_revenue_id,dim_revenue,data):\n",
    "    #existing_action = dim_revenue.filter(col(\"TICKER\") == data[\"company_name\"]).count()\n",
    "    # Check and add new rows to the dimension table  brand\n",
    "    existing_action=0\n",
    "    if existing_action == 0:\n",
    "        new_revenue_row = {\n",
    "        \"ID_REVENUE\":new_revenue_id,\n",
    "        \"REVENUE\":\"\",\n",
    "        \"REVENUE_PER_SHARE\":\"\",\n",
    "        \"QUARTERLY_REVENUE_GROWTH\":\"\",\n",
    "        \"GROSS_PROFIT\":\"\",\n",
    "        \"NET_INCOME\":\"\",\n",
    "        \"QUARTERLY_EARNINGS_GROWTH\":\"\",\n",
    "        \"TOTAL_CASH\":\"\",\n",
    "        \"TOTAL_CASH_PER_SHARE\":\"\",\n",
    "        \"TOTAL_DEBT\":\"\",\n",
    "        \"TOTAL_DEBT_EQUITY\":\"\",\n",
    "        \"CURRENT_RATIO\":\"\",\n",
    "        \"TICKER\":data[\"company_name\"]}\n",
    "        # create the new line for the update\n",
    "        new_dim_revenue =spark.createDataFrame([new_revenue_row])\n",
    "        # Write the updated dimension data back to the respective table\n",
    "        new_dim_revenue.write.format(\"jdbc\").option(\"driver\", driver).option(\"url\", url).option(\"dbtable\", \"DIM_REVENUE_TTM\").mode(\"append\").save()\n",
    "#checkandaddidrevenue(new_revenue_id,dim_revenue,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e51ff1f3-0e68-4ac8-8e97-f7dadbef7d01",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_id_info(data,dim_info): \n",
    "    num1= dim_info.filter(col(\"SYMBOL\") == data[\"company_name\"]).count()\n",
    "    if num1==0 :\n",
    "      new_info_id =dim_info.selectExpr(\"MAX(ID_INFO_ACTION) + 1\").first()[0]\n",
    "    else :\n",
    "      new_info_id = dim_info.filter(col(\"SYMBOL\") == data[\"company_name\"]).select(\"ID_INFO_ACTION\").first()[0]\n",
    "    return new_info_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5db5b1a5-8f59-48d7-987a-7abd1e52d791",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_id_revenue(data,dim_revenue): \n",
    "    num1= dim_revenue.filter(col(\"TICKER\") == data[\"company_name\"]).count()\n",
    "    if num1==0 :\n",
    "      new_revenue_id =dim_revenue.selectExpr(\"MAX(ID_REVENUE) + 1\").first()[0]\n",
    "    else :\n",
    "      new_revenue_id = dim_revenue.filter(col(\"TICKER\") == data[\"company_name\"]).select(\"ID_REVENUE\").first()[0]\n",
    "    return new_revenue_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd12db14-65fb-4d0d-a397-17a5e2adfbbc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_id_fact(data, table_fait):\n",
    "    num1 = table_fait.filter(\n",
    "        (col(\"ID_DATE\") == data[\"Date\"]) &\n",
    "        (col(\"CLOSE\") == data[\"Close\"]) &\n",
    "        (col(\"OPEN\") == data[\"Open\"]) &\n",
    "        (col(\"HIGHT\") == data[\"High\"]) &\n",
    "        (col(\"LOW\") == data[\"Low\"]) &\n",
    "        (col(\"VOLUME\") == data[\"Volume\"]) &\n",
    "        (col(\"ADJ_CLOSE\") == data[\"Adj Close\"])\n",
    "    ).count()\n",
    "\n",
    "    if num1 == 0:\n",
    "        new_fact_id = table_fait.selectExpr(\"MAX(ID_ACTION) + 1\").first()[0]\n",
    "    else:\n",
    "        #raise Exception(\"Operation canceled. Conditions already exist in table_fait.\")\n",
    "        return\n",
    "    return new_fact_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f859c2d7-cf11-4502-a80c-7e095ec06535",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generaterow(data):\n",
    "    \n",
    "    dim_date = (spark.read.format(\"jdbc\").option(\"driver\", driver).option(\"url\", url).option(\"dbtable\", \"DIM_DATE\").load())\n",
    "    dim_info= (spark.read.format(\"jdbc\").option(\"driver\", driver).option(\"url\", url).option(\"dbtable\", \"DIM_INFO_ACT\").load())\n",
    "    dim_revenue= (spark.read.format(\"jdbc\").option(\"driver\", driver).option(\"url\", url).option(\"dbtable\", \"DIM_REVENUE_TTM\").load())\n",
    "    table_fait = (spark.read.format(\"jdbc\").option(\"driver\", driver).option(\"url\", url).option(\"dbtable\", \"fact_table\").load())\n",
    "    #dim_info.show()\n",
    "    #dim_revenue\n",
    "    new_info_id =create_id_info(data,dim_info)\n",
    "    new_revenue_id =create_id_revenue(data,dim_revenue)\n",
    "    dim_date = dim_date.withColumnRenamed(\"ID_DATE\", \"NEW_ID_DATE\")\n",
    "    dim_info = dim_info.withColumnRenamed(\"ID_INFO_ACTION\", \"NEW_ID_INFO_ACTION\")\n",
    "    dim_revenue = dim_revenue.withColumnRenamed(\"ID_REVENUE\", \"NEW_ID_REVENUE\")\n",
    "    factID=create_id_fact(data, table_fait)\n",
    "    new_date_id= data['Date']\n",
    "    new_row = {\n",
    "      \"ID_ACTION\":factID,\n",
    "      \"ID_DATE\": data[\"Date\"],\n",
    "      \"ID_INFO_ACTION\":new_info_id,\n",
    "      \"ID_DIVIDEND\":1,\n",
    "      \"OPEN\":data[\"Open\"],\n",
    "      \"HIGHT\":data[\"High\"],\n",
    "      \"LOW\":data[\"Low\"],\n",
    "      \"CLOSE\":data[\"Close\"],\n",
    "      \"VOLUME\": data[\"Volume\"],\n",
    "      \"ADJ_CLOSE\":data[\"Adj Close\"],\n",
    "      \"ID_REVENUE\":new_revenue_id\n",
    "    }    \n",
    "    new_row_df = spark.createDataFrame([new_row])\n",
    "    # Perform a left join on each dimension table with the new row data to check for existing SKs\n",
    "    new_row_df = spark.createDataFrame([new_row])\n",
    "    new_row_df = new_row_df.join(dim_date, new_row_df.ID_DATE == dim_date.NEW_ID_DATE, \"left\")\n",
    "    new_row_df = new_row_df.join(dim_info, new_row_df.ID_INFO_ACTION == dim_info.NEW_ID_INFO_ACTION, \"left\")\n",
    "    new_row_df = new_row_df.join(dim_revenue, new_row_df.ID_REVENUE == dim_revenue.NEW_ID_REVENUE, \"left\")\n",
    "   \n",
    "    #new_row_df.display()\n",
    "    checkandaddskdate(new_date_id, dim_date, data) \n",
    "    new_row_df1=new_row_df\n",
    "    new_row_df = new_row_df \\\n",
    "    .withColumn(\"ID_DATE\", when(col(\"NEW_ID_DATE\").isNull(), lit(data[\"Date\"])).otherwise(col(\"NEW_ID_DATE\"))) \\\n",
    "    .withColumn(\"ID_INFO_ACTION\", when(col(\"NEW_ID_INFO_ACTION\").isNull(), lit(new_info_id)).otherwise(col(\"NEW_ID_INFO_ACTION\")))\\\n",
    "    .withColumn(\"ID_REVENUE\", when(col(\"NEW_ID_REVENUE\").isNull(), lit(new_revenue_id)).otherwise(col(\"NEW_ID_REVENUE\")))\n",
    "    \n",
    "    # Select only the required columns for the Fact_product table and append the new row to the existing data\n",
    "    new_row_df = new_row_df.select(\"ID_ACTION\",\"ID_DATE\",\"ID_INFO_ACTION\", \"ID_DIVIDEND\",\"OPEN\",\"HIGHT\", \"LOW\",\"CLOSE\",\"VOLUME\" ,\n",
    "                                   \"ADJ_CLOSE\", \"ID_REVENUE\")\n",
    "    dim_date = dim_date.withColumnRenamed(\"NEW_ID_DATE\", \"ID_DATE\")\n",
    "    if new_row_df.collect()[0][2]==new_info_id:\n",
    "        checkandaddidinfoaction(new_row_df.collect()[0][2],dim_info,data)\n",
    "    if new_row_df.collect()[0][10]==new_revenue_id:\n",
    "        checkandaddidrevenue(new_row_df.collect()[0][10],dim_revenue,data)\n",
    "    # Save the DataFrame to the database table\n",
    "     \n",
    "    num_rows = new_row_df.count()\n",
    "    print(num_rows)\n",
    "    if ( num_rows >1 ):\n",
    "      first_row = new_row_df.limit(1)\n",
    "    else :\n",
    "      first_row=new_row_df\n",
    "    \n",
    "    existing_row = table_fait.filter(\n",
    "        (col(\"ID_DATE\") == new_row_df.collect()[0][1]) &\n",
    "        (col(\"ID_INFO_ACTION\") == new_row_df.collect()[0][2])\n",
    "    ).count() \n",
    "    if existing_row == 0:\n",
    "         first_row.write.format(\"jdbc\").option('url', url).option(\"dbtable\", \"fact_table\").mode(\"append\").save()\n",
    "    else:\n",
    "        #raise Exception(\"Operation canceled. Conditions already exist in table_fait.\")\n",
    "        \n",
    "        return print(\"row already exist\")\n",
    "    return new_row_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f340ea2d-333f-41b6-bb6d-a85235cb3f0c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import asyncio\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "from pyspark.sql.functions import col,lit,when\n",
    "import pyodbc\n",
    "import time\n",
    "from azure.eventhub.aio import EventHubConsumerClient\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Define the schema of the JSON\n",
    "json_schema = StructType([\n",
    "    StructField(\"Date\", StringType()),\n",
    "    StructField(\"Open\", DoubleType()),\n",
    "    StructField(\"High\", DoubleType()),\n",
    "    StructField(\"Low\", DoubleType()),\n",
    "    StructField(\"Close\", DoubleType()),\n",
    "    StructField(\"Adj Close\", DoubleType()),\n",
    "    StructField(\"Volume\", IntegerType()),\n",
    "    StructField(\"company_name\", StringType())\n",
    "])\n",
    "\n",
    "cs = 'Endpoint=sb://eventstream.servicebus.windows.net/;SharedAccessKeyName=event;SharedAccessKey=PQ2PYqSCXTIy2zee6qc1aHKTKH2rgi5pf+AEhBsYa+Q=;EntityPath=eventhub1'\n",
    "connection_string = cs + \";EntityPath=eventhub1\"\n",
    "\n",
    "conf = {}\n",
    "conf['eventhubs.connectionString'] = sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(connection_string)\n",
    "conf['eventhubs.consumerGroup'] = \"$Default\"\n",
    "\n",
    "# Load streaming data from eventhubs\n",
    "input_stream_df = spark.readStream.format(\"eventhubs\").options(**conf).option(\"startingOffsets\", \"earliest\").load()\n",
    "\n",
    "# Convert the \"body\" column to JSON structure\n",
    "body_df = input_stream_df.select(from_json(col(\"body\").cast(\"string\"), json_schema).alias(\"body_json\"))\n",
    "\n",
    "# Extract the JSON fields into separate columns\n",
    "body_df = body_df.select(\n",
    "    col(\"body_json.Date\"),\n",
    "    col(\"body_json.Open\"),\n",
    "    col(\"body_json.High\"),\n",
    "    col(\"body_json.Low\"),\n",
    "    col(\"body_json.Close\"),\n",
    "    col(\"body_json.`Adj Close`\").alias(\"Adj_Close\"),\n",
    "    col(\"body_json.Volume\"),\n",
    "    col(\"body_json.company_name\")\n",
    ")\n",
    "\n",
    "# Define the process_event function to handle events\n",
    "async def process_event(partition_context, event):\n",
    "    # Extract the data from the event body\n",
    "    event_body = ''.join(chunk.decode('utf-8') for chunk in event.body)\n",
    "    print(\"Event Body:\", event_body)\n",
    "    \n",
    "    # Convert the event body to a JSON object\n",
    "    event_data = json.loads(event_body)\n",
    "    \n",
    "    # Generate the new row DataFrame\n",
    "    new_row_df = generaterow(event_data)\n",
    "    new_row_df.show(truncate=False\n",
    "                   )\n",
    "    \n",
    "    # Save the DataFrame to the database table\n",
    "    #new_row_df.write.format(\"jdbc\").option('url', url).option(\"dbtable\", \"fait_test\").mode(\"append\").save()\n",
    "\n",
    "#print(\"Send messages in {} seconds.\".format(time.time() - start_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03b87021-41cd-46dc-aa75-4adf75d38852",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_fait = (spark.read.format(\"jdbc\").option(\"driver\", driver).option(\"url\", url).option(\"dbtable\", \"fact_table\").load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c5226a4-0996-4517-83b7-de95fcc6f865",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import asyncio\n",
    "# Start consuming events from the Event Hub\n",
    "client = EventHubConsumerClient.from_connection_string(\n",
    "    conn_str=cs,\n",
    "    consumer_group=\"$Default\",\n",
    "     eventhub_name=\"eventhub1\"\n",
    "     )\n",
    "async def consume_events():\n",
    "    await client.receive(\n",
    "        on_event=process_event,\n",
    "        starting_position=\"@latest\",  # Start from the beginning of the event stream\n",
    "    )\n",
    "\n",
    "await consume_events()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a097445-4c71-4719-9fda-6c74246aa05c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4181463285483916,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "EventHub_Consumer",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

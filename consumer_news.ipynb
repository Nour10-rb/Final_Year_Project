{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f85aae85-02a5-4d1c-811a-6718355bf23b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import yfinance as yf\n",
    "yf.pdr_override()\n",
    "\n",
    "\n",
    "#import fix_yahoo_finance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9cacbe5-8de5-45cf-af82-77745005c929",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1150fcc-01e5-4772-a721-8642c20d71fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "driver = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "\n",
    "database_host = \"portefeuille.database.windows.net\"\n",
    "database_port = \"1433\" # update if you use a non-default port\n",
    "database_name = \"portefeuille\"\n",
    "table = \"DIM_DATE\"\n",
    "user = \"portefeuille\"\n",
    "password = \"Noursharm_123\"\n",
    "url=\"jdbc:sqlserver://portefeuille.database.windows.net:1433;database=portefeuille;user=portefeuille@portefeuille;password=Noursharm_123;encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.database.windows.net;loginTimeout=30;\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b44e31f2-a0d7-4ffe-b035-41aa0f4582a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_event = (spark.read.format(\"jdbc\").option(\"driver\", driver).option(\"url\", url).option(\"dbtable\", \"event_sentiment\").load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5724c66c-017c-400e-a1cd-70821cab2929",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[16]: True"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d4adfd4-ebe5-4c6f-acd3-25c58a6539e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = {'Stock': 'MSFT','Event_Name': 'UPDATE 1-Twitter is refusing to pay its Google Cloud bills- Platformer'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fefdf18a-f539-489a-a71d-0dae89245e81",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Instantiate the SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define a UDF (User-Defined Function) to apply sentiment analysis\n",
    "def get_sentiment(text):\n",
    "    sentiment_score = sia.polarity_scores(text)['compound']\n",
    "    if sentiment_score > 0:\n",
    "        return 'Positive'\n",
    "    elif sentiment_score < 0:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "\n",
    "#data = table_event_with_sentiment.select('STOCK' ,'EVENT_NAME','Sentiment')\n",
    "#data.write.format(\"jdbc\").option(\"driver\", driver).option(\"url\", url).option(\"dbtable\", \"event_sentiment\").mode(\"append\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2226b55-0690-4a6a-b0eb-540b427eec29",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,lit,when\n",
    "def generaterow(data):    \n",
    "    table_event = (spark.read.format(\"jdbc\").option(\"driver\", driver).option(\"url\", url).option(\"dbtable\", \"event_sentiment\").load())\n",
    "    new_row = {\n",
    "      \"stock\":data['Stock'],\n",
    "      \"event_name\": data[\"Event_Name\"],\n",
    "      \"sentiment\":get_sentiment(data['Event_Name']),\n",
    "    }   \n",
    "    new_row_df = spark.createDataFrame([new_row])\n",
    "    existing_row = table_event.filter(\n",
    "        (col(\"stock\") == new_row_df.collect()[0][2]) &\n",
    "        (col(\"event_name\") == new_row_df.collect()[0][0]) \n",
    "    ).count() \n",
    "    new_row_df.show()\n",
    "    if existing_row == 0:\n",
    "         new_row_df.write.format(\"jdbc\").option('url', url).option(\"dbtable\", \"event_sentiment\").mode(\"append\").save()\n",
    "    else:\n",
    "        #raise Exception(\"Operation canceled. Conditions already exist in table_fait.\")\n",
    "        \n",
    "        return print(\"row already exist\")\n",
    "    \n",
    "    return new_row_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03b79aa5-017b-4f74-9de3-acb8a7b4142d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import asyncio\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "from pyspark.sql.functions import col,lit,when\n",
    "import pyodbc\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bc2b0e4-413f-488f-b1fa-3398e17e7a43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import asyncio\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "from pyspark.sql.functions import col,lit,when\n",
    "import pyodbc\n",
    "import time\n",
    "from azure.eventhub.aio import EventHubConsumerClient\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "# Define the schema of the JSON\n",
    "json_schema = StructType([\n",
    "    StructField(\"Stock\", StringType()),\n",
    "    StructField(\"Event_Name\", StringType())\n",
    "])\n",
    "\n",
    "\n",
    "cs = 'Endpoint=sb://evenetstream.servicebus.windows.net/;SharedAccessKeyName=streamdata;SharedAccessKey=Hfd1AYOfca/hX5J7/EbtPcTySlWOUFesH+AEhK64OQY=;EntityPath=eventhub1'\n",
    "connection_string = cs + \";EntityPath=eventhub1\"\n",
    "\n",
    "conf = {}\n",
    "conf['eventhubs.connectionString'] = sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(connection_string)\n",
    "conf['eventhubs.consumerGroup'] = \"$Default\"\n",
    "\n",
    "# Load streaming data from eventhubs\n",
    "input_stream_df = spark.readStream.format(\"eventhubs\").options(**conf).option(\"startingOffsets\", \"earliest\").load()\n",
    "\n",
    "# Convert the \"body\" column to JSON structure\n",
    "body_df = input_stream_df.select(from_json(col(\"body\").cast(\"string\"), json_schema).alias(\"body_json\"))\n",
    "\n",
    "# Extract the JSON fields into separate columns\n",
    "body_df = body_df.select(\n",
    "    col(\"body_json.Stock\").alias(\"Adj_Close\"),\n",
    "    col(\"body_json.Event_Name\").alias(\"Adj_Close\")\n",
    ")\n",
    "\n",
    "# Define the process_event function to handle events\n",
    "async def process_event(partition_context, event):\n",
    "    # Extract the data from the event body\n",
    "    event_body = ''.join(chunk.decode('utf-8') for chunk in event.body)\n",
    "    print(\"Event Body:\", event_body)\n",
    "    \n",
    "    # Convert the event body to a JSON object\n",
    "    event_data = json.loads(event_body)\n",
    "    \n",
    "    # Generate the new row DataFrame\n",
    "    new_row_df = generaterow(event_data)\n",
    "    new_row_df.show(truncate=False\n",
    "                   )\n",
    "    \n",
    "    # Save the DataFrame to the database table\n",
    "    #new_row_df.write.format(\"jdbc\").option('url', url).option(\"dbtable\", \"fait_test\").mode(\"append\").save()\n",
    "\n",
    "#print(\"Send messages in {} seconds.\".format(time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef6ce0d2-f191-40a7-9b0c-14efb0396ad9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event Body: {\"Stock\": \"AAPL\", \"Event_Name\": \"These Stocks Are Moving the Most Today: Oracle, Tesla, Apple, GameStop, and More\"}\n+--------------------+---------+-----+\n|          event_name|sentiment|stock|\n+--------------------+---------+-----+\n|These Stocks Are ...|  Neutral| AAPL|\n+--------------------+---------+-----+\n\n+--------------------------------------------------------------------------------+---------+-----+\n|event_name                                                                      |sentiment|stock|\n+--------------------------------------------------------------------------------+---------+-----+\n|These Stocks Are Moving the Most Today: Oracle, Tesla, Apple, GameStop, and More|Neutral  |AAPL |\n+--------------------------------------------------------------------------------+---------+-----+\n\nEvent Body: {\"Stock\": \"MSFT\", \"Event_Name\": \"A US judge temporarily blocked Microsoft's Activision Blizzard deal\"}\n+--------------------+---------+-----+\n|          event_name|sentiment|stock|\n+--------------------+---------+-----+\n|A US judge tempor...| Negative| MSFT|\n+--------------------+---------+-----+\n\nrow already exist\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EventProcessor instance '5e92fd71-661f-4b8a-aeee-bb74cb569e7c' of eventhub 'eventhub1' partition '0' consumer group '$Default'. An error occurred while receiving. The exception is AttributeError(\"'NoneType' object has no attribute 'show'\").\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event Body: {\"Stock\": \"MSFT\", \"Event_Name\": \"How to Invest in Artificial Intelligence (AI) Technology\"}\n+--------------------+---------+-----+\n|          event_name|sentiment|stock|\n+--------------------+---------+-----+\n|How to Invest in ...| Positive| MSFT|\n+--------------------+---------+-----+\n\n+--------------------------------------------------------+---------+-----+\n|event_name                                              |sentiment|stock|\n+--------------------------------------------------------+---------+-----+\n|How to Invest in Artificial Intelligence (AI) Technology|Positive |MSFT |\n+--------------------------------------------------------+---------+-----+\n\nEvent Body: {\"Stock\": \"GOOG\", \"Event_Name\": \"70% of Companies Will Use AI by 2030 -- 2 Super Stocks to Buy Right Now\"}\n+--------------------+---------+-----+\n|          event_name|sentiment|stock|\n+--------------------+---------+-----+\n|70% of Companies ...| Positive| GOOG|\n+--------------------+---------+-----+\n\n+-----------------------------------------------------------------------+---------+-----+\n|event_name                                                             |sentiment|stock|\n+-----------------------------------------------------------------------+---------+-----+\n|70% of Companies Will Use AI by 2030 -- 2 Super Stocks to Buy Right Now|Positive |GOOG |\n+-----------------------------------------------------------------------+---------+-----+\n\nEvent Body: {\"Stock\": \"AAL\", \"Event_Name\": \"Analyst Report: American Airlines Group Inc.\"}\n+--------------------+---------+-----+\n|          event_name|sentiment|stock|\n+--------------------+---------+-----+\n|Analyst Report: A...|  Neutral|  AAL|\n+--------------------+---------+-----+\n\n+--------------------------------------------+---------+-----+\n|event_name                                  |sentiment|stock|\n+--------------------------------------------+---------+-----+\n|Analyst Report: American Airlines Group Inc.|Neutral  |AAL  |\n+--------------------------------------------+---------+-----+\n\nEvent Body: {\"Stock\": \"AAL\", \"Event_Name\": \"Top 20 Most Profitable Airlines in the World\"}\n+--------------------+---------+-----+\n|          event_name|sentiment|stock|\n+--------------------+---------+-----+\n|Top 20 Most Profi...| Positive|  AAL|\n+--------------------+---------+-----+\n\n+--------------------------------------------+---------+-----+\n|event_name                                  |sentiment|stock|\n+--------------------------------------------+---------+-----+\n|Top 20 Most Profitable Airlines in the World|Positive |AAL  |\n+--------------------------------------------+---------+-----+\n\nEvent Body: {\"Stock\": \"AMZN\", \"Event_Name\": \"If I Could Only Buy 1 AI Stock Right Now, This Would Be It\"}\n+--------------------+---------+-----+\n|          event_name|sentiment|stock|\n+--------------------+---------+-----+\n|If I Could Only B...|  Neutral| AMZN|\n+--------------------+---------+-----+\n\n+----------------------------------------------------------+---------+-----+\n|event_name                                                |sentiment|stock|\n+----------------------------------------------------------+---------+-----+\n|If I Could Only Buy 1 AI Stock Right Now, This Would Be It|Neutral  |AMZN |\n+----------------------------------------------------------+---------+-----+\n\nEvent Body: {\"Stock\": \"ICE\", \"Event_Name\": \"Here's Why We're Wary Of Buying Intercontinental Exchange's (NYSE:ICE) For Its Upcoming Dividend\"}\n+--------------------+---------+-----+\n|          event_name|sentiment|stock|\n+--------------------+---------+-----+\n|Here's Why We're ...|  Neutral|  ICE|\n+--------------------+---------+-----+\n\n+------------------------------------------------------------------------------------------------+---------+-----+\n|event_name                                                                                      |sentiment|stock|\n+------------------------------------------------------------------------------------------------+---------+-----+\n|Here's Why We're Wary Of Buying Intercontinental Exchange's (NYSE:ICE) For Its Upcoming Dividend|Neutral  |ICE  |\n+------------------------------------------------------------------------------------------------+---------+-----+\n\nEvent Body: {\"Stock\": \"GOOG\", \"Event_Name\": \"Alphabet, United Airlines, and 4 Other Stocks That Still Have Room to Run\"}\n+--------------------+---------+-----+\n|          event_name|sentiment|stock|\n+--------------------+---------+-----+\n|Alphabet, United ...| Positive| GOOG|\n+--------------------+---------+-----+\n\n+-------------------------------------------------------------------------+---------+-----+\n|event_name                                                               |sentiment|stock|\n+-------------------------------------------------------------------------+---------+-----+\n|Alphabet, United Airlines, and 4 Other Stocks That Still Have Room to Run|Positive |GOOG |\n+-------------------------------------------------------------------------+---------+-----+\n\nEvent Body: {\"Stock\": \"MSFT\", \"Event_Name\": \"NVIDIA Corporation (NVDA) Benefitted from its Exposure to Artificial Intelligence\"}\n+--------------------+---------+-----+\n|          event_name|sentiment|stock|\n+--------------------+---------+-----+\n|NVIDIA Corporatio...| Positive| MSFT|\n+--------------------+---------+-----+\n\n+---------------------------------------------------------------------------------+---------+-----+\n|event_name                                                                       |sentiment|stock|\n+---------------------------------------------------------------------------------+---------+-----+\n|NVIDIA Corporation (NVDA) Benefitted from its Exposure to Artificial Intelligence|Positive |MSFT |\n+---------------------------------------------------------------------------------+---------+-----+\n\nEvent Body: {\"Stock\": \"GOOG\", \"Event_Name\": \"A.I. is changing business and society faster than anyone expected. These 13 A.I. innovators are deciding how the tech will change your life\"}\n+--------------------+---------+-----+\n|          event_name|sentiment|stock|\n+--------------------+---------+-----+\n|A.I. is changing ...|  Neutral| GOOG|\n+--------------------+---------+-----+\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EventProcessor instance '5e92fd71-661f-4b8a-aeee-bb74cb569e7c' of eventhub 'eventhub1' partition '0' consumer group '$Default'. An error occurred while receiving. The exception is Py4JJavaError('An error occurred while calling o1857.save.\\n', JavaObject id=o1858).\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import asyncio\n",
    "# Start consuming events from the Event Hub\n",
    "client = EventHubConsumerClient.from_connection_string(\n",
    "    conn_str=cs,\n",
    "    consumer_group=\"$Default\",\n",
    "     eventhub_name=\"eventhub1\"\n",
    "     )\n",
    "async def consume_events():\n",
    "    await client.receive(\n",
    "        on_event=process_event,\n",
    "        starting_position=\"@latest\",  # Start from the beginning of the event stream\n",
    "    )\n",
    "\n",
    "await consume_events()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a9759c0-1178-4ad4-856b-4dea9e2df360",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "consumer_news",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
